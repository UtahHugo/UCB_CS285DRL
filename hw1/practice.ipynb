{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hong/anaconda3/envs/cs285/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from cs285.infrastructure import pytorch_util as ptu\n",
    "from cs285.infrastructure import utils\n",
    "from cs285.infrastructure.logger import Logger\n",
    "from cs285.infrastructure.replay_buffer import ReplayBuffer\n",
    "from cs285.policies.MLP_policy import MLPPolicySL\n",
    "from cs285.policies.loaded_gaussian_policy import LoadedGaussianPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NVIDEO = 2\n",
    "MAX_VIDEO_LEN = 40  # we overwrite this in the code below\n",
    "\n",
    "MJ_ENV_NAMES = [\"Ant-v4\", \"Walker2d-v4\", \"HalfCheetah-v4\", \"Hopper-v4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--expert_policy_file', '-epf', type=str, required=True)  # relative to where you're running this script from\n",
    "parser.add_argument('--expert_data', '-ed', type=str, required=True) #relative to where you're running this script from\n",
    "parser.add_argument('--env_name', '-env', type=str, help=f'choices: {\", \".join(MJ_ENV_NAMES)}', required=True)\n",
    "parser.add_argument('--exp_name', '-exp', type=str, default='pick an experiment name', required=True)\n",
    "parser.add_argument('--do_dagger', action='store_true')\n",
    "parser.add_argument('--ep_len', type=int)\n",
    "\n",
    "parser.add_argument('--num_agent_train_steps_per_iter', type=int, default=1000)  # number of gradient steps for training policy (per iter in n_iter)\n",
    "parser.add_argument('--n_iter', '-n', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--batch_size', type=int, default=1000)  # training data collected (in the env) during each iteration\n",
    "parser.add_argument('--eval_batch_size', type=int,\n",
    "                    default=1000)  # eval data collected (in the env) for logging metrics\n",
    "parser.add_argument('--train_batch_size', type=int,\n",
    "                    default=100)  # number of sampled data points to be used per gradient/train step\n",
    "\n",
    "parser.add_argument('--n_layers', type=int, default=2)  # depth, of policy to be learned\n",
    "parser.add_argument('--size', type=int, default=64)  # width of each layer, of policy to be learned\n",
    "parser.add_argument('--learning_rate', '-lr', type=float, default=5e-3)  # LR for supervised learning\n",
    "\n",
    "parser.add_argument('--video_log_freq', type=int, default=5)\n",
    "parser.add_argument('--scalar_log_freq', type=int, default=1)\n",
    "parser.add_argument('--no_gpu', '-ngpu', action='store_true')\n",
    "parser.add_argument('--which_gpu', type=int, default=0)\n",
    "parser.add_argument('--max_replay_buffer_size', type=int, default=1000000)\n",
    "parser.add_argument('--save_params', action='store_true')\n",
    "parser.add_argument('--seed', type=int, default=1)\n",
    "\n",
    "input = [\"--expert_policy_file\", 'cs285/policies/experts/Ant.pkl', \"--expert_data\", 'cs285/expert_data/expert_data_Ant-v4.pkl', \n",
    "         \"--env_name\", \"Ant-v4\", \"--exp_name\", \"bc_ant\"]\n",
    "args = parser.parse_args(args=input)\n",
    "params = vars(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 40\n",
    "env = gym.make(params['env_name'], render_mode=None)\n",
    "env.reset(seed=seed)\n",
    "\n",
    "# Maximum length for episodes\n",
    "params['ep_len'] = params['ep_len'] or env.spec.max_episode_steps\n",
    "MAX_VIDEO_LEN = params['ep_len']\n",
    "\n",
    "assert isinstance(env.action_space, gym.spaces.Box), \"Environment must be continuous\"\n",
    "# Observation and action sizes\n",
    "ob_dim = env.observation_space.shape[0]\n",
    "ac_dim = env.action_space.shape[0]\n",
    "\n",
    "# simulation timestep, will be used for video saving\n",
    "if 'model' in dir(env):\n",
    "    fps = 1/env.model.opt.timestep\n",
    "else:\n",
    "    fps = env.env.metadata['render_fps']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'logdir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m logger \u001b[38;5;241m=\u001b[39m Logger(\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogdir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Set random seeds\u001b[39;00m\n\u001b[1;32m      4\u001b[0m seed \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'logdir'"
     ]
    }
   ],
   "source": [
    "logger = Logger(params['logdir'])\n",
    "\n",
    "# Set random seeds\n",
    "seed = params['seed']\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "ptu.init_gpu(\n",
    "    use_gpu=not params['no_gpu'],\n",
    "    gpu_id=params['which_gpu']\n",
    ")\n",
    "\n",
    "# Set logger attributes\n",
    "log_video = True\n",
    "log_metrics = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import itertools\n",
    "from typing import Any\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import distributions\n",
    "\n",
    "from cs285.infrastructure import pytorch_util as ptu\n",
    "from cs285.policies.base_policy import BasePolicy\n",
    "\n",
    "\n",
    "def build_mlp(\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        n_layers: int,\n",
    "        size: int\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "        Builds a feedforward neural network\n",
    "\n",
    "        arguments:\n",
    "            n_layers: number of hidden layers\n",
    "            size: dimension of each hidden layer\n",
    "            activation: activation of each hidden layer\n",
    "\n",
    "            input_size: size of the input layer\n",
    "            output_size: size of the output layer\n",
    "            output_activation: activation of the output layer\n",
    "\n",
    "        returns:\n",
    "            MLP (nn.Module)\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    in_size = input_size\n",
    "    for _ in range(n_layers):\n",
    "        layers.append(nn.Linear(in_size, size))\n",
    "        layers.append(nn.Tanh())\n",
    "        in_size = size\n",
    "    layers.append(nn.Linear(in_size, output_size))\n",
    "\n",
    "    mlp = nn.Sequential(*layers)\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPolicySL(BasePolicy, nn.Module, metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    Defines an MLP for supervised learning which maps observations to continuous\n",
    "    actions.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    mean_net: nn.Sequential\n",
    "        A neural network that outputs the mean for continuous actions\n",
    "    logstd: nn.Parameter\n",
    "        A separate parameter to learn the standard deviation of actions\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward:\n",
    "        Runs a differentiable forwards pass through the network\n",
    "    update:\n",
    "        Trains the policy with a supervised learning objective\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 ac_dim,\n",
    "                 ob_dim,\n",
    "                 n_layers,\n",
    "                 size,\n",
    "                 learning_rate=1e-4,\n",
    "                 training=True,\n",
    "                 nn_baseline=False,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # init vars\n",
    "        self.ac_dim = ac_dim\n",
    "        self.ob_dim = ob_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.size = size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.training = training\n",
    "        self.nn_baseline = nn_baseline\n",
    "\n",
    "        self.mean_net = build_mlp(\n",
    "            input_size=self.ob_dim,\n",
    "            output_size=self.ac_dim,\n",
    "            n_layers=self.n_layers, size=self.size,\n",
    "        )\n",
    "        self.mean_net.to(ptu.device)\n",
    "        self.logstd = nn.Parameter(\n",
    "\n",
    "            torch.zeros(self.ac_dim, dtype=torch.float32, device=ptu.device)\n",
    "        )\n",
    "        self.logstd.to(ptu.device)\n",
    "        self.optimizer = optim.Adam(\n",
    "            itertools.chain([self.logstd], self.mean_net.parameters()),\n",
    "            self.learning_rate\n",
    "        )\n",
    "\n",
    "    def save(self, filepath):\n",
    "        \"\"\"\n",
    "        :param filepath: path to save MLP\n",
    "        \"\"\"\n",
    "        torch.save(self.state_dict(), filepath)\n",
    "\n",
    "    def forward(self, observation: torch.FloatTensor) -> Any:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network\n",
    "\n",
    "        :param observation: observation(s) to query the policy\n",
    "        :return:\n",
    "            action: sampled action(s) from the policy\n",
    "        \"\"\"\n",
    "        # TODO: implement the forward pass of the network.\n",
    "        # You can return anything you want, but you should be able to differentiate\n",
    "        # through it. For example, you can return a torch.FloatTensor. You can also\n",
    "        # return more flexible objects, such as a\n",
    "        # `torch.distributions.Distribution` object. It's up to you!\n",
    "\n",
    "        action_mean = self.mean_net(observation)\n",
    "        action_logstd = self.logstd\n",
    "        action_dist = torch.distributions.normal.Normal(action_mean, torch.exp(action_logstd))\n",
    "        \n",
    "        return action_dist\n",
    "\n",
    "\n",
    "    def update(self, observations, actions):\n",
    "        \"\"\"\n",
    "        Updates/trains the policy\n",
    "\n",
    "        :param observations: observation(s) to query the policy\n",
    "        :param actions: actions we want the policy to imitate\n",
    "        :return:\n",
    "            dict: 'Training Loss': supervised learning loss\n",
    "        \"\"\"\n",
    "        # TODO: update the policy and return the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = torch.sum((self.forward(observations).sample(actions.shape[0]) - actions)**2)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            # You can add extra logging information here, but keep this line\n",
    "            'Training Loss': ptu.to_numpy(loss),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = MLPPolicySL(\n",
    "    ac_dim,\n",
    "    ob_dim,\n",
    "    params['n_layers'],\n",
    "    params['size'],\n",
    "    learning_rate=params['learning_rate'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading expert policy from... cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(params['max_replay_buffer_size'])\n",
    "\n",
    "#######################\n",
    "## LOAD EXPERT POLICY\n",
    "#######################\n",
    "\n",
    "print('Loading expert policy from...', params['expert_policy_file'])\n",
    "expert_policy = LoadedGaussianPolicy(params['expert_policy_file'])\n",
    "expert_policy.to(ptu.device)\n",
    "print('Done restoring expert policy...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectory(env, policy, max_path_length, render=False):\n",
    "    \"\"\"Sample a rollout in the environment from a policy.\"\"\"\n",
    "    \n",
    "    # initialize env for the beginning of a new rollout\n",
    "    ob =  env.reset() # TODO: initial observation after resetting the env\n",
    "\n",
    "    # init vars\n",
    "    obs, acs, rewards, next_obs, terminals, image_obs = [], [], [], [], [], []\n",
    "    steps = 0\n",
    "    while True:\n",
    "\n",
    "        # render image of the simulated env\n",
    "        if render:\n",
    "            if hasattr(env, 'sim'):\n",
    "                img = env.sim.render(camera_name='track', height=500, width=500)[::-1]\n",
    "            else:\n",
    "                img = env.render(mode='single_rgb_array')\n",
    "            image_obs.append(cv2.resize(img, dsize=(250, 250), interpolation=cv2.INTER_CUBIC))\n",
    "    \n",
    "        # TODO use the most recent ob to decide what to do\n",
    "        ac = policy(ob).sample() # HINT: this is a numpy array\n",
    "        ac = ac[0]\n",
    "\n",
    "        # TODO: take that action and get reward and next ob\n",
    "        next_ob, rew, done, _ = env.step(ac)\n",
    "        \n",
    "        # TODO rollout can end due to done, or due to max_path_length\n",
    "        steps += 1\n",
    "        rollout_done = int(done or steps >= max_path_length) # HINT: this is either 0 or 1\n",
    "        \n",
    "        # record result of taking that action\n",
    "        obs.append(ob)\n",
    "        acs.append(ac)\n",
    "        rewards.append(rew)\n",
    "        next_obs.append(next_ob)\n",
    "        terminals.append(rollout_done)\n",
    "\n",
    "        ob = next_ob # jump to next timestep\n",
    "\n",
    "        # end the rollout if the rollout ended\n",
    "        if rollout_done:\n",
    "            break\n",
    "\n",
    "    return {\"observation\" : np.array(obs, dtype=np.float32),\n",
    "            \"image_obs\" : np.array(image_obs, dtype=np.uint8),\n",
    "            \"reward\" : np.array(rewards, dtype=np.float32),\n",
    "            \"action\" : np.array(acs, dtype=np.float32),\n",
    "            \"next_observation\": np.array(next_obs, dtype=np.float32),\n",
    "            \"terminal\": np.array(terminals, dtype=np.float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msample_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m, in \u001b[0;36msample_trajectory\u001b[0;34m(env, policy, max_path_length, render)\u001b[0m\n\u001b[1;32m     18\u001b[0m     image_obs\u001b[38;5;241m.\u001b[39mappend(cv2\u001b[38;5;241m.\u001b[39mresize(img, dsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m250\u001b[39m, \u001b[38;5;241m250\u001b[39m), interpolation\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_CUBIC))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# TODO use the most recent ob to decide what to do\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m ac \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mob\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample() \u001b[38;5;66;03m# HINT: this is a numpy array\u001b[39;00m\n\u001b[1;32m     22\u001b[0m ac \u001b[38;5;241m=\u001b[39m ac[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# TODO: take that action and get reward and next ob\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 77\u001b[0m, in \u001b[0;36mMLPPolicySL.forward\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mDefines the forward pass of the network\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    action: sampled action(s) from the policy\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# TODO: implement the forward pass of the network.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# You can return anything you want, but you should be able to differentiate\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# through it. For example, you can return a torch.FloatTensor. You can also\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# return more flexible objects, such as a\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# `torch.distributions.Distribution` object. It's up to you!\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m action_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m action_logstd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogstd\n\u001b[1;32m     79\u001b[0m action_dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mnormal\u001b[38;5;241m.\u001b[39mNormal(action_mean, torch\u001b[38;5;241m.\u001b[39mexp(action_logstd))\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285/lib/python3.9/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "sample_trajectory(env, actor, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9715774 ,  0.7519275 ,  0.3866682 ,  1.4880048 , -0.40164468,\n",
       "        0.6947728 , -0.13829237, -1.4387997 ], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob =  env.reset()\n",
    "np.array(actor(torch.tensor(ob[0], dtype=torch.float32)).sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8238,  0.9941,  0.0979, -0.0219, -0.0408,  0.0161,  0.0706, -0.0384,\n",
       "         0.0448, -0.0244,  0.0012, -0.0785,  0.0741, -0.0177,  0.0309, -0.0092,\n",
       "        -0.0965, -0.1108,  0.0053,  0.0418, -0.0473, -0.1283,  0.0810,  0.0263,\n",
       "        -0.1864, -0.1090,  0.1192], dtype=torch.float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(ob[0], dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7.96861620e-01,  9.94054782e-01,  6.09826866e-02, -5.65110720e-02,\n",
       "        -7.03043401e-02, -7.21872158e-02, -1.40094501e-03, -9.58007719e-02,\n",
       "        -9.68530448e-02,  6.37948411e-02, -4.38224707e-02, -9.88979949e-02,\n",
       "        -2.74399558e-04, -7.05942358e-02, -4.69545353e-02, -5.99215471e-02,\n",
       "        -1.72873846e-01,  5.28312850e-03,  2.43504421e-02, -2.30624374e-01,\n",
       "        -6.31208015e-02,  5.48171028e-02,  5.99633659e-02, -1.12662131e-01,\n",
       "         3.37050015e-02, -1.14437962e-01, -5.91980448e-02]),\n",
       " {})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob =  env.reset()\n",
    "ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
