{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hong/anaconda3/envs/cs285/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from cs285.infrastructure import pytorch_util as ptu\n",
    "from cs285.infrastructure import utils\n",
    "from cs285.infrastructure.logger import Logger\n",
    "from cs285.infrastructure.replay_buffer import ReplayBuffer\n",
    "from cs285.policies.MLP_policy import MLPPolicySL\n",
    "from cs285.policies.loaded_gaussian_policy import LoadedGaussianPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NVIDEO = 2\n",
    "MAX_VIDEO_LEN = 40  # we overwrite this in the code below\n",
    "\n",
    "MJ_ENV_NAMES = [\"Ant-v4\", \"Walker2d-v4\", \"HalfCheetah-v4\", \"Hopper-v4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--expert_policy_file', '-epf', type=str, required=True)  # relative to where you're running this script from\n",
    "parser.add_argument('--expert_data', '-ed', type=str, required=True) #relative to where you're running this script from\n",
    "parser.add_argument('--env_name', '-env', type=str, help=f'choices: {\", \".join(MJ_ENV_NAMES)}', required=True)\n",
    "parser.add_argument('--exp_name', '-exp', type=str, default='pick an experiment name', required=True)\n",
    "parser.add_argument('--do_dagger', action='store_true')\n",
    "parser.add_argument('--ep_len', type=int)\n",
    "\n",
    "parser.add_argument('--num_agent_train_steps_per_iter', type=int, default=1000)  # number of gradient steps for training policy (per iter in n_iter)\n",
    "parser.add_argument('--n_iter', '-n', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--batch_size', type=int, default=1000)  # training data collected (in the env) during each iteration\n",
    "parser.add_argument('--eval_batch_size', type=int,\n",
    "                    default=1000)  # eval data collected (in the env) for logging metrics\n",
    "parser.add_argument('--train_batch_size', type=int,\n",
    "                    default=100)  # number of sampled data points to be used per gradient/train step\n",
    "\n",
    "parser.add_argument('--n_layers', type=int, default=2)  # depth, of policy to be learned\n",
    "parser.add_argument('--size', type=int, default=64)  # width of each layer, of policy to be learned\n",
    "parser.add_argument('--learning_rate', '-lr', type=float, default=5e-3)  # LR for supervised learning\n",
    "\n",
    "parser.add_argument('--video_log_freq', type=int, default=5)\n",
    "parser.add_argument('--scalar_log_freq', type=int, default=1)\n",
    "parser.add_argument('--no_gpu', '-ngpu', action='store_true')\n",
    "parser.add_argument('--which_gpu', type=int, default=0)\n",
    "parser.add_argument('--max_replay_buffer_size', type=int, default=1000000)\n",
    "parser.add_argument('--save_params', action='store_true')\n",
    "parser.add_argument('--seed', type=int, default=1)\n",
    "\n",
    "input = [\"--expert_policy_file\", 'cs285/policies/experts/Ant.pkl', \"--expert_data\", 'cs285/expert_data/expert_data_Ant-v4.pkl', \n",
    "         \"--env_name\", \"Ant-v4\", \"--exp_name\", \"bc_ant\"]\n",
    "args = parser.parse_args(args=input)\n",
    "params = vars(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 40\n",
    "env = gym.make(params['env_name'], render_mode=None)\n",
    "env.reset(seed=seed)\n",
    "\n",
    "# Maximum length for episodes\n",
    "params['ep_len'] = params['ep_len'] or env.spec.max_episode_steps\n",
    "MAX_VIDEO_LEN = params['ep_len']\n",
    "\n",
    "assert isinstance(env.action_space, gym.spaces.Box), \"Environment must be continuous\"\n",
    "# Observation and action sizes\n",
    "ob_dim = env.observation_space.shape[0]\n",
    "ac_dim = env.action_space.shape[0]\n",
    "\n",
    "# simulation timestep, will be used for video saving\n",
    "if 'model' in dir(env):\n",
    "    fps = 1/env.model.opt.timestep\n",
    "else:\n",
    "    fps = env.env.metadata['render_fps']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'logdir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m logger \u001b[38;5;241m=\u001b[39m Logger(\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogdir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Set random seeds\u001b[39;00m\n\u001b[1;32m      4\u001b[0m seed \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'logdir'"
     ]
    }
   ],
   "source": [
    "logger = Logger(params['logdir'])\n",
    "\n",
    "# Set random seeds\n",
    "seed = params['seed']\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "ptu.init_gpu(\n",
    "    use_gpu=not params['no_gpu'],\n",
    "    gpu_id=params['which_gpu']\n",
    ")\n",
    "\n",
    "# Set logger attributes\n",
    "log_video = True\n",
    "log_metrics = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import itertools\n",
    "from typing import Any\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import distributions\n",
    "\n",
    "from cs285.infrastructure import pytorch_util as ptu\n",
    "from cs285.policies.base_policy import BasePolicy\n",
    "\n",
    "\n",
    "def build_mlp(\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        n_layers: int,\n",
    "        size: int\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "        Builds a feedforward neural network\n",
    "\n",
    "        arguments:\n",
    "            n_layers: number of hidden layers\n",
    "            size: dimension of each hidden layer\n",
    "            activation: activation of each hidden layer\n",
    "\n",
    "            input_size: size of the input layer\n",
    "            output_size: size of the output layer\n",
    "            output_activation: activation of the output layer\n",
    "\n",
    "        returns:\n",
    "            MLP (nn.Module)\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    in_size = input_size\n",
    "    for _ in range(n_layers):\n",
    "        layers.append(nn.Linear(in_size, size))\n",
    "        layers.append(nn.Tanh())\n",
    "        in_size = size\n",
    "    layers.append(nn.Linear(in_size, output_size))\n",
    "\n",
    "    mlp = nn.Sequential(*layers)\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPolicySL(BasePolicy, nn.Module, metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    Defines an MLP for supervised learning which maps observations to continuous\n",
    "    actions.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    mean_net: nn.Sequential\n",
    "        A neural network that outputs the mean for continuous actions\n",
    "    logstd: nn.Parameter\n",
    "        A separate parameter to learn the standard deviation of actions\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward:\n",
    "        Runs a differentiable forwards pass through the network\n",
    "    update:\n",
    "        Trains the policy with a supervised learning objective\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 ac_dim,\n",
    "                 ob_dim,\n",
    "                 n_layers,\n",
    "                 size,\n",
    "                 learning_rate=1e-4,\n",
    "                 training=True,\n",
    "                 nn_baseline=False,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # init vars\n",
    "        self.ac_dim = ac_dim\n",
    "        self.ob_dim = ob_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.size = size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.training = training\n",
    "        self.nn_baseline = nn_baseline\n",
    "\n",
    "        self.mean_net = build_mlp(\n",
    "            input_size=self.ob_dim,\n",
    "            output_size=self.ac_dim,\n",
    "            n_layers=self.n_layers, size=self.size,\n",
    "        )\n",
    "        self.mean_net.to(ptu.device)\n",
    "        self.logstd = nn.Parameter(\n",
    "\n",
    "            torch.zeros(self.ac_dim, dtype=torch.float32, device=ptu.device)\n",
    "        )\n",
    "        self.logstd.to(ptu.device)\n",
    "        self.optimizer = optim.Adam(\n",
    "            itertools.chain([self.logstd], self.mean_net.parameters()),\n",
    "            self.learning_rate\n",
    "        )\n",
    "\n",
    "    def save(self, filepath):\n",
    "        \"\"\"\n",
    "        :param filepath: path to save MLP\n",
    "        \"\"\"\n",
    "        torch.save(self.state_dict(), filepath)\n",
    "\n",
    "    def forward(self, observation: torch.FloatTensor) -> Any:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network\n",
    "\n",
    "        :param observation: observation(s) to query the policy\n",
    "        :return:\n",
    "            action: sampled action(s) from the policy\n",
    "        \"\"\"\n",
    "        # TODO: implement the forward pass of the network.\n",
    "        # You can return anything you want, but you should be able to differentiate\n",
    "        # through it. For example, you can return a torch.FloatTensor. You can also\n",
    "        # return more flexible objects, such as a\n",
    "        # `torch.distributions.Distribution` object. It's up to you!\n",
    "\n",
    "        action_mean = self.mean_net(observation)\n",
    "        action_logstd = self.logstd\n",
    "        action_dist = torch.distributions.normal.Normal(action_mean, torch.exp(action_logstd))\n",
    "        \n",
    "        return action_dist\n",
    "\n",
    "\n",
    "    def update(self, observations, actions):\n",
    "        \"\"\"\n",
    "        Updates/trains the policy\n",
    "\n",
    "        :param observations: observation(s) to query the policy\n",
    "        :param actions: actions we want the policy to imitate\n",
    "        :return:\n",
    "            dict: 'Training Loss': supervised learning loss\n",
    "        \"\"\"\n",
    "        # TODO: update the policy and return the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = torch.sum((self.forward(observations).sample(actions.shape[0]) - actions)**2)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            # You can add extra logging information here, but keep this line\n",
    "            'Training Loss': ptu.to_numpy(loss),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = MLPPolicySL(\n",
    "    ac_dim,\n",
    "    ob_dim,\n",
    "    params['n_layers'],\n",
    "    params['size'],\n",
    "    learning_rate=params['learning_rate'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading expert policy from... cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(params['max_replay_buffer_size'])\n",
    "\n",
    "#######################\n",
    "## LOAD EXPERT POLICY\n",
    "#######################\n",
    "\n",
    "print('Loading expert policy from...', params['expert_policy_file'])\n",
    "expert_policy = LoadedGaussianPolicy(params['expert_policy_file'])\n",
    "expert_policy.to(ptu.device)\n",
    "print('Done restoring expert policy...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectory(env, policy, max_path_length, render=False):\n",
    "    \"\"\"Sample a rollout in the environment from a policy.\"\"\"\n",
    "    \n",
    "    # initialize env for the beginning of a new rollout\n",
    "    ob =  env.reset() # TODO: initial observation after resetting the env\n",
    "\n",
    "    # init vars\n",
    "    obs, acs, rewards, next_obs, terminals, image_obs = [], [], [], [], [], []\n",
    "    steps = 0\n",
    "    while True:\n",
    "\n",
    "        # render image of the simulated env\n",
    "        if render:\n",
    "            if hasattr(env, 'sim'):\n",
    "                img = env.sim.render(camera_name='track', height=500, width=500)[::-1]\n",
    "            else:\n",
    "                img = env.render(mode='single_rgb_array')\n",
    "            image_obs.append(cv2.resize(img, dsize=(250, 250), interpolation=cv2.INTER_CUBIC))\n",
    "    \n",
    "        # TODO use the most recent ob to decide what to do\n",
    "        ac = ptu.to_numpy(policy(ptu.from_numpy(ob[0])).sample()) # HINT: this is a numpy array\n",
    "        # ac = ac[0]\n",
    "\n",
    "        # TODO: take that action and get reward and next ob\n",
    "        next_ob, rew, done, _ = env.step(ac)\n",
    "        \n",
    "        # TODO rollout can end due to done, or due to max_path_length\n",
    "        steps += 1\n",
    "        rollout_done = int(done or steps >= max_path_length) # HINT: this is either 0 or 1\n",
    "        \n",
    "        # record result of taking that action\n",
    "        obs.append(ob)\n",
    "        acs.append(ac)\n",
    "        rewards.append(rew)\n",
    "        next_obs.append(next_ob)\n",
    "        terminals.append(rollout_done)\n",
    "\n",
    "        ob = next_ob # jump to next timestep\n",
    "\n",
    "        # end the rollout if the rollout ended\n",
    "        if rollout_done:\n",
    "            break\n",
    "\n",
    "    return {\"observation\" : np.array(obs, dtype=np.float32),\n",
    "            \"image_obs\" : np.array(image_obs, dtype=np.uint8),\n",
    "            \"reward\" : np.array(rewards, dtype=np.float32),\n",
    "            \"action\" : np.array(acs, dtype=np.float32),\n",
    "            \"next_observation\": np.array(next_obs, dtype=np.float32),\n",
    "            \"terminal\": np.array(terminals, dtype=np.float32)}\n",
    "\n",
    "\n",
    "def sample_trajectories(env, policy, min_timesteps_per_batch, max_path_length, render=False):\n",
    "    \"\"\"Collect rollouts until we have collected min_timesteps_per_batch steps.\"\"\"\n",
    "\n",
    "    timesteps_this_batch = 0\n",
    "    paths = []\n",
    "    while timesteps_this_batch < min_timesteps_per_batch:\n",
    "\n",
    "        #collect rollout\n",
    "        path = sample_trajectory(env, policy, max_path_length, render)\n",
    "        paths.append(path)\n",
    "\n",
    "        #count steps\n",
    "        timesteps_this_batch += get_pathlength(path)\n",
    "\n",
    "    return paths, timesteps_this_batch\n",
    "\n",
    "\n",
    "def sample_n_trajectories(env, policy, ntraj, max_path_length, render=False):\n",
    "    \"\"\"Collect ntraj rollouts.\"\"\"\n",
    "\n",
    "    paths = []\n",
    "    for i in range(ntraj):\n",
    "        # collect rollout\n",
    "        path = sample_trajectory(env, policy, max_path_length, render)\n",
    "        paths.append(path)\n",
    "    return paths\n",
    "\n",
    "\n",
    "########################################\n",
    "########################################\n",
    "\n",
    "\n",
    "def convert_listofrollouts(paths, concat_rew=True):\n",
    "    \"\"\"\n",
    "        Take a list of rollout dictionaries\n",
    "        and return separate arrays,\n",
    "        where each array is a concatenation of that array from across the rollouts\n",
    "    \"\"\"\n",
    "    observations = np.concatenate([path[\"observation\"] for path in paths])\n",
    "    actions = np.concatenate([path[\"action\"] for path in paths])\n",
    "    if concat_rew:\n",
    "        rewards = np.concatenate([path[\"reward\"] for path in paths])\n",
    "    else:\n",
    "        rewards = [path[\"reward\"] for path in paths]\n",
    "    next_observations = np.concatenate([path[\"next_observation\"] for path in paths])\n",
    "    terminals = np.concatenate([path[\"terminal\"] for path in paths])\n",
    "    return observations, actions, rewards, next_observations, terminals\n",
    "\n",
    "\n",
    "########################################\n",
    "########################################\n",
    "            \n",
    "\n",
    "def compute_metrics(paths, eval_paths):\n",
    "    \"\"\"Compute metrics for logging.\"\"\"\n",
    "\n",
    "    # returns, for logging\n",
    "    train_returns = [path[\"reward\"].sum() for path in paths]\n",
    "    eval_returns = [eval_path[\"reward\"].sum() for eval_path in eval_paths]\n",
    "\n",
    "    # episode lengths, for logging\n",
    "    train_ep_lens = [len(path[\"reward\"]) for path in paths]\n",
    "    eval_ep_lens = [len(eval_path[\"reward\"]) for eval_path in eval_paths]\n",
    "\n",
    "    # decide what to log\n",
    "    logs = OrderedDict()\n",
    "    logs[\"Eval_AverageReturn\"] = np.mean(eval_returns)\n",
    "    logs[\"Eval_StdReturn\"] = np.std(eval_returns)\n",
    "    logs[\"Eval_MaxReturn\"] = np.max(eval_returns)\n",
    "    logs[\"Eval_MinReturn\"] = np.min(eval_returns)\n",
    "    logs[\"Eval_AverageEpLen\"] = np.mean(eval_ep_lens)\n",
    "\n",
    "    logs[\"Train_AverageReturn\"] = np.mean(train_returns)\n",
    "    logs[\"Train_StdReturn\"] = np.std(train_returns)\n",
    "    logs[\"Train_MaxReturn\"] = np.max(train_returns)\n",
    "    logs[\"Train_MinReturn\"] = np.min(train_returns)\n",
    "    logs[\"Train_AverageEpLen\"] = np.mean(train_ep_lens)\n",
    "\n",
    "    return logs\n",
    "\n",
    "\n",
    "############################################\n",
    "############################################\n",
    "\n",
    "\n",
    "def get_pathlength(path):\n",
    "    return len(path[\"reward\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(params['env_name'], render_mode=None)\n",
    "env.reset(seed=seed)\n",
    "\n",
    "# Maximum length for episodes\n",
    "params['ep_len'] = params['ep_len'] or env.spec.max_episode_steps\n",
    "MAX_VIDEO_LEN = params['ep_len']\n",
    "\n",
    "assert isinstance(env.action_space, gym.spaces.Box), \"Environment must be continuous\"\n",
    "# Observation and action sizes\n",
    "ob_dim = env.observation_space.shape[0]\n",
    "ac_dim = env.action_space.shape[0]\n",
    "\n",
    "# simulation timestep, will be used for video saving\n",
    "if 'model' in dir(env):\n",
    "    fps = 1/env.model.opt.timestep\n",
    "else:\n",
    "    fps = env.env.metadata['render_fps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7.5252e-01,  9.9678e-01, -3.4393e-02, -5.3391e-02, -4.8943e-02,\n",
      "        -5.4545e-02, -4.1537e-02,  4.0942e-02, -7.1095e-02,  2.3804e-02,\n",
      "        -2.8939e-04, -6.1984e-02, -3.4497e-02,  1.6083e-01,  6.9057e-02,\n",
      "        -2.5030e-01,  1.2590e-01,  2.0117e-01, -2.6759e-02,  8.1855e-02,\n",
      "        -8.5196e-02,  5.1258e-03,  8.5344e-03, -6.2477e-02,  9.0801e-02,\n",
      "         5.4587e-02,  6.4104e-02])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hong/anaconda3/envs/cs285/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mep_len\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/UCB_CS285DRL/hw1/cs285/infrastructure/utils.py:76\u001b[0m, in \u001b[0;36msample_trajectories\u001b[0;34m(env, policy, min_timesteps_per_batch, max_path_length, render)\u001b[0m\n\u001b[1;32m     72\u001b[0m paths \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m timesteps_this_batch \u001b[38;5;241m<\u001b[39m min_timesteps_per_batch:\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m#collect rollout\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[43msample_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_path_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(path)\n\u001b[1;32m     78\u001b[0m     paths\u001b[38;5;241m.\u001b[39mappend(path)\n",
      "File \u001b[0;32m~/UCB_CS285DRL/hw1/cs285/infrastructure/utils.py:41\u001b[0m, in \u001b[0;36msample_trajectory\u001b[0;34m(env, policy, max_path_length, render)\u001b[0m\n\u001b[1;32m     37\u001b[0m ac \u001b[38;5;241m=\u001b[39m ptu\u001b[38;5;241m.\u001b[39mto_numpy(policy(ptu\u001b[38;5;241m.\u001b[39mfrom_numpy(ob[\u001b[38;5;241m0\u001b[39m]))\u001b[38;5;241m.\u001b[39msample()) \u001b[38;5;66;03m# HINT: this is a numpy array\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# ac = ac[0]\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# TODO: take that action and get reward and next ob\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m next_ob, rew, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(ac)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# TODO rollout can end due to done, or due to max_path_length\u001b[39;00m\n\u001b[1;32m     44\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "utils.sample_trajectories(env, actor, params['batch_size'], params['ep_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ac \u001b[38;5;241m=\u001b[39m ptu\u001b[38;5;241m.\u001b[39mto_numpy(actor(ptu\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mob\u001b[49m[\u001b[38;5;241m0\u001b[39m]))\u001b[38;5;241m.\u001b[39msample())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ob' is not defined"
     ]
    }
   ],
   "source": [
    "ac = ptu.to_numpy(actor(ptu.from_numpy(ob[0])).sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02477547, 0.77494264, 1.2743918 , 1.5708604 , 0.41619882,\n",
       "       2.7236302 , 0.53704965, 2.287044  ], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
